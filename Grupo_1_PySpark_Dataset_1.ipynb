{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GRUPO 1 PySpark Dataset 1\n",
        "\n",
        "Nicolasa Nancy Illanes Castillo\n",
        "\n",
        "Gerick Toro\n",
        "\n",
        "Juan Jose Velarde\n"
      ],
      "metadata": {
        "id": "0-52sBJFlqqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vCCIRQ61FMEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495b39ff-b51c-475c-893a-bcc81773a0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler\n",
        "\n",
        "\n",
        "# Inicializar Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataQualityExample\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cargar datos\n",
        "df_spark = spark.read.option(\"header\", \"true\").csv(\"data.csv\", inferSchema=True)\n",
        "df_spark.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analisis de calidad\n",
        "\n",
        "def spark_quality_analysis(df):\n",
        "\n",
        "    print(\" ANÁLISIS CALIDAD DATOS   \")\n",
        "\n",
        "    # Conteo de registros\n",
        "    print(f\"Numero de total registros: {df.count()}\")\n",
        "    print(f\"Total columnas: {len(df.columns)}\")\n",
        "\n",
        "    # Análisis de nulls por columna\n",
        "    null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
        "        for c in df.columns  ]).collect()[0]\n",
        "\n",
        "    print(\"\\nVALORES NULOS \")\n",
        "    for col_name, null_count in null_counts.asDict().items():\n",
        "        if null_count > 0:\n",
        "            percentage = (null_count / df.count()) * 100\n",
        "            print(f\"{col_name}: {null_count} ({percentage:.2f}%)\")\n",
        "\n",
        "    # Duplicados\n",
        "    total_rows = df.count()\n",
        "    unique_rows = df.distinct().count()\n",
        "    duplicates = total_rows - unique_rows\n",
        "    print(f\"\\nDUPLICADOS: {duplicates}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "spark_quality_analysis(df_spark)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhOFP3DYpo8h",
        "outputId": "8b740022-900f-4865-ac37-8dccc5f945a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ANÁLISIS CALIDAD DATOS   \n",
            "Numero de total registros: 541909\n",
            "Total columnas: 8\n",
            "\n",
            "VALORES NULOS \n",
            "Description: 1454 (0.27%)\n",
            "CustomerID: 135080 (24.93%)\n",
            "\n",
            "DUPLICADOS: 5268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza Distribuida Sapark\n",
        "\n",
        "def spark_data_cleaning(df):\n",
        "    \"\"\"Pipeline de limpieza para Spark\"\"\"\n",
        "\n",
        "    # 1. Eliminar registros con muchos nulls\n",
        "    threshold = 0.5  # Eliminar filas con >50% nulls\n",
        "    min_non_null = int(threshold * len(df.columns))\n",
        "    df_clean = df.dropna(thresh=min_non_null)\n",
        "\n",
        "    # 2. Imputación por columna\n",
        "    numeric_cols = [field.name for field in df_clean.schema.fields\n",
        "                   if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
        "\n",
        "    # Imputar con mediana\n",
        "    for col_name in numeric_cols:\n",
        "        median_val = df_clean.approxQuantile(col_name, [0.5], 0.01)[0]\n",
        "        df_clean = df_clean.na.fill({col_name: median_val})\n",
        "\n",
        "    # Imputar categóricas con moda\n",
        "    string_cols = [field.name for field in df_clean.schema.fields\n",
        "                  if field.dataType == StringType()]\n",
        "\n",
        "\n",
        "    for col_name in string_cols:\n",
        "        print(col_name)\n",
        "        mode_val = df_clean.filter(col(col_name).isNotNull()).groupBy(col_name).count().orderBy(desc(\"count\")).first()[0]\n",
        "        df_clean = df_clean.na.fill({col_name: mode_val})\n",
        "\n",
        "    # 3. Eliminar duplicados\n",
        "    df_clean = df_clean.dropDuplicates()\n",
        "\n",
        "    # 4. Detección de outliers usando IQR\n",
        "    for col_name in numeric_cols:\n",
        "        quantiles = df_clean.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
        "        q1, q3 = quantiles[0], quantiles[1]\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        df_clean = df_clean.filter(\n",
        "            (col(col_name) >= lower_bound) &\n",
        "            (col(col_name) <= upper_bound)\n",
        "        )\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "\n",
        "df_spark_clean = spark_data_cleaning(df_spark)\n",
        "print(f\"Registros después de limpieza: {df_spark_clean.count()}\")\n",
        "df_spark_clean.show(5)\n"
      ],
      "metadata": {
        "id": "VozvOoXoGSwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37443a9-a020-4956-e56c-3574eabd64d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvoiceNo\n",
            "StockCode\n",
            "Description\n",
            "InvoiceDate\n",
            "Country\n",
            "Registros después de limpieza: 468260\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|   536367|    22745|POPPY'S PLAYHOUSE...|       6| 12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
            "|   536368|    22960|JAM MAKING SET WI...|       6| 12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
            "|   536388|    22915|ASSORTED BOTTLE T...|      12| 12/1/2010 9:59|     0.42|     16250|United Kingdom|\n",
            "|   536401|    21464|DISCO BALL ROTATO...|       1|12/1/2010 11:21|     4.25|     15862|United Kingdom|\n",
            "|   536412|    22569|FELTCRAFT CUSHION...|       2|12/1/2010 11:49|     3.75|     17920|United Kingdom|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
